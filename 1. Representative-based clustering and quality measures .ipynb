{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "warming-sweden",
   "metadata": {},
   "source": [
    "# Representative-based Clustering with sklearn: An introduction\n",
    "Prepared and compiled by: Marissa Liponhay\n",
    "\n",
    "Representative-based clustering is all about grouping datasets or instances into the desired number of clusters centered on a selected representative point, which may or may not be an actual data point.  The goal is to have instances in cluster to be more similar to one another than to the instances in another cluster. The general algorithm starts by selecting random representatives. The clustering algorithm generally starts with a random selection of 'representatives'.The remaining datapoints will then be assigned to the nearest representative point based on their 'distances' from the representative point, which are measured by distance or dissimilarity measures. The process of selecting representative points and assignment of remaining data is repeated until the convergence criteria is met. A convergence is achieved if there are no more changes in the assignment of individuals.\n",
    "\n",
    "The general algorithm follows the following steps:\n",
    "\n",
    "**Algorithm** *GenericRepresentative*(Database: $D$, Number of representatives: $k$)  \n",
    "**begin**  \n",
    "$\\quad$Initialize representative set $S$;  \n",
    "$\\quad$$\\quad$**repeat**  \n",
    "$\\quad$$\\quad$Create clusters ($C_1, ..., C_k$) by assigning each point in $D$ to closest representative in $S$ using the distance function $Dist(\\cdot,\\cdot)$  \n",
    "$\\quad$$\\quad$Recreate set $S$ by determining one representative $y_j$ for each $C_j$ that minimizes $\\sum_{x_i \\in C_j} Dist(x_i, y_j)$  \n",
    "$\\quad$**until** convergence;  \n",
    "$\\quad$**return** $(C_i, ..., C_k)$;  \n",
    "**end**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-undergraduate",
   "metadata": {},
   "source": [
    "As an illustration, let us consider a 1-D system. The algorithm proceeds as follows:\n",
    "Step 1. Set k as the number of clusters you want to make. Then select k random initial representatives which will serve as the center of clusters.\n",
    "<img src = \"figures\\1.png\" width = \"500\"/>\n",
    "<img src = \"figures\\2.png\" width = \"500\"/>\n",
    "\n",
    "\n",
    "Step 2. Calculate the distances of all instances or data points from the selected representatives.\n",
    "<img src = \"figures\\3.png\" width = \"500\"/>\n",
    "\n",
    "\n",
    "Step 3. Assign the instances to the nearest cluster representative.\n",
    "<img src = \"figures\\4.png\" width = \"500\"/>\n",
    "<img src = \"figures\\5.png\" width = \"500\"/>\n",
    "<img src = \"figures\\6.png\" width = \"500\"/>\n",
    "\n",
    "\n",
    "Step 4. For each cluster, select a new cluster representative or center that leads to a lower total distance of the cluster (from the center) and repeat steps 1-3 with respect to the new center until the assignment of cluster does not change in the succeeding steps\n",
    "\n",
    "The process can be easily extended to higher dimensions where data points are vectors. Shown below are illustrations of the steps 1 to step 4 in 2D after re-selection of centers.\n",
    "\n",
    "Step 1. Randomly initialize centers.\n",
    "<img src = \"figures\\rand_center_new.PNG\" width = \"500\"/>\n",
    "\n",
    "Step 2. Calculate the distances of all the points from the center.\n",
    "<img src = \"figures\\assignment_new.PNG\" width = \"500\"/>\n",
    "\n",
    "Step 3. Select a new center that reduces cost of the whole dataset clustering.\n",
    "<img src = \"figures\\recentered_new.PNG\" width = \"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-chess",
   "metadata": {},
   "source": [
    "Among the representative-based clustering algorithms that are discussed in this chapter are listed below.\n",
    "1. k-means\n",
    "2. k-medians\n",
    "3. k-medoids\n",
    "4. k-modes \n",
    "5. Clara\n",
    "6. Clarans\n",
    "\n",
    "The representative-based clustering methods are mostly different from the criteria being applied in step 4. Fortunately, sklearn allows implementations of algorithms 1-3, while kmodes and pyclustering packages are available for the implementation and k-modes and Clarans respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-botswana",
   "metadata": {},
   "source": [
    "The summary of the differences between different representative-based clustering algorithm:\n",
    "<img src ='figures/comparison.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-genealogy",
   "metadata": {},
   "source": [
    "Since representative-based clustering cannot really tell us the quality of clusters, we need validation measures on the results of the clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-mouse",
   "metadata": {},
   "source": [
    "## Internal Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-change",
   "metadata": {},
   "source": [
    "Internal validation means we are not relying on ground truth values to evaluate the quality of the clustering.  Each criterion will favor a clustering method that optimizes a function which is similar to the criterion itself. Thus, comparing totally different clustering methods using a criterion may end up being misleading.\n",
    "\n",
    "The internal validation criteria that we will use are:\n",
    "\n",
    "* **Sum of squares distances to centroids**: In the case of $k$-means and other representative-based methods, this is $Q$ and corresponds to the sum of squares distances to the representative points. This is given by the `inertia_` property of `KMeans`. Smaller values suggest better clustering. This is optimized for distance-based algorithms.\n",
    "\n",
    "* **Calinski-Harabasz index**: Also known as the Variance Ratio Criterion, it is the ratio of the between-clusters dispersion mean and the within-cluster dispersion,\n",
    "    $$s_k = \\frac{B_k/(k-1)}{W_k/(n-k)}$$\n",
    "    where $B_k$ is the between group dispersion matrix and $W_k$ is the within-cluster dispersion matrix defined by\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    B_k &= \\sum_{i=1}^k n_i \\lvert\\lvert \\bar x_i - \\bar x \\rvert\\rvert_2^2;\\\\\n",
    "    W_k &= \\sum_{i=1}^k \\sum_{x \\in C_i} \\lvert\\lvert x - \\bar x_i \\rvert\\rvert_2^2,\n",
    "    \\end{align}\n",
    "    $$\n",
    "    with $n$ the number of points in our data, $C_i$ the set of points in cluster $i$, $\\bar x_i$ the center of cluster $i$, $\\bar x$ be the center of all points, $n_i$ be the number of points in cluster $i$ and $n$ be the number of points. The higher the value of this measure, the more defined the clusters are.\n",
    "\n",
    "* **Intracluster to intercluster distance ratio**: We sample $r$ pairs of data points from the underlying data. We then take the ratio of the average distance of pairs $P$ that belong to the same cluster, and the average distance of pairs $Q$ that belong to different clusters,\n",
    "$$\n",
    "Intra/Inter = \\frac{\\sum_{(x_i, x_j) \\in P)} Dist(x_i, x_j)/\\left|P\\right|}{\\sum_{(x_i, x_j) \\in Q)} Dist(x_i, x_j)/\\left|Q\\right|}.\n",
    "$$\n",
    "Small values of this measure indicate better clustering behavior.\n",
    "\n",
    "* **Silhouette coefficient**: Let $Davg^{in}_i$ be the average distance of $x_i$ to data points within the cluster of $x_i$, and $Dmin^{out}_i$ be the smallest average distance to points other than its own cluster. The silhouette coefficient $S_i$ for the $i$th data point specific to the ith object, is as follows: \n",
    "    $$\n",
    "    S_i = \\frac{Dmin^{out}_i − Davg^{in}_i}{\\max\\{{Dmin^{out}_i}, Davg^{in}_i\\}}\n",
    "    $$\n",
    "    The overall silhouette coefficient is the average of $S_i$. Its value is in the the range (−1,1). Large positive values indicate highly separated clustering, and negative values are indicative of some level of “mixing” of data points from different clusters. This is because $Dmin^{out}_i$ will be less than $Davg^{in}_i$ only in cases where data point $x_i$ is closer to at least one other cluster than its own cluster. One advantage of this coefficient is that the absolute values provide a good intuitive feel of the quality of the clustering.\n",
    "\n",
    "* **Gap statistic**: This compares the observed within-cluster sum-of-squared-distances with a reference null distribution generated using a Monte Carlo method approach: reference clusterings are created from $b$ realizations of the data. The generated feature value of the reference distribution can be drawn from \n",
    "  * a uniform distribution over the range of observed values for that feature;\n",
    "  * a uniform distribution over a box aligned with the principal components of the data.\n",
    "  \n",
    "  We will only implement the first option.\n",
    "  \n",
    "  To compute the gap statistic for $k$ clusters, let us first define the pooled within-cluster sum of squares around the cluster mean,\n",
    "  $$\\bar{W}_k = \\sum_{i=1}^k \\sum_{x \\in C_i} \\frac{1}{2n_i} \\lvert\\lvert x - \\bar x_i \\rvert\\rvert_2^2.$$\n",
    "  This is similar to $W_k$ above but the cluster size is used to normalize the sum-of-squared-distances for each cluster. We then define the gap statistic as\n",
    "  $$\\text{Gap}_n(k) = \\frac{1}{b} \\sum_i^b \\log(\\bar{W}_{k,i}) - \\log(\\bar{W}_k),$$\n",
    "  where $\\bar{W}_{k,i}$ is the $\\bar{W}_{k}$ of the $i$th realization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-prediction",
   "metadata": {},
   "source": [
    "## External Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-psychology",
   "metadata": {},
   "source": [
    "External validation requires ground truth labels and in real life, these are hard to come by. However, since we assumed a ground truth label based on the cultivator and newsgroup, we can use these criteria.\n",
    "\n",
    "The external validation measures that we will compute are:\n",
    "* **Cluster purity**: Measures how dominant the dominant class of each ground truth class is. The higher the value the better. Let $m_{ij}$ be the number of data points from ground-truth cluster $i$ that are mapped to cluster $j$, $N_i$ is the number of data points in the ground-truth class $i$, $M_j$ the number of data points in cluster $j$, $k_t$ is the number of ground truth clusters and $k_d$ is the number of clusters determined by the algorithm. We can then write the number of data points in different clusters as\n",
    "$$\n",
    "N_i = \\sum_{j=1}^{k_d} m_{ij}; \\\\\n",
    "M_j = \\sum_{i=1}^{k_t} m_{ij}.\n",
    "$$\n",
    "The number of data points $P_j$ in the dominant class of cluster $j$ is given by\n",
    "$$\n",
    "P_j = \\max_i m_{ij}.\n",
    "$$\n",
    "We define Purity as\n",
    "$$\n",
    "\\text{Purity} = \\frac{\\sum_{j=1}^{k_d} P_j}{\\sum_{j=1}^{k_d} M_j}.\n",
    "$$\n",
    "High values of Purity are desirable.\n",
    "* **Adjusted mutual information**: A problem of Purity is that it only considers the dominant class. Adjusted mutual information considers the distribution of all classes. Unlike other mutual information measures, it is also adjusted for chance. For two class labelings $U$ and $V$, the adjusted mutual information is\n",
    "$$\n",
    "\\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\max(H[U], H[V]) - E[\\text{MI}]}.\n",
    "$$\n",
    "The expectation $E[\\text{MI}]$ is quite long and can be found [here](https://en.wikipedia.org/wiki/Adjusted_mutual_information#Adjustment_for_chance). Values of AMI is in $[0,1]$ with values close to zero implying the two labelings are independent and a value of 1 indicating that the two label assignments are equal (with or without permutation).\n",
    "* **Adjusted Rand Index**: The adjusted Rand index is another measure that considers all classes, not just the dominant class. The Rand Index is the sum of the fractions of pairs of elements that are in the same class and should be in the same class, and pairs of elements that are in different classes and should be in different classes. If $C$ is the ground-truth class assignment and $K$ is the clustering, we define $a$ and $b$ as:\n",
    "  * $a$: the number of pairs of elements that are in the same set in $C$ and in the same set in $K$\n",
    "  * $b$: the number of pairs of elements that are in different sets in $C$ and in different sets in $K$\n",
    "The Rand index is then given by\n",
    "$$\n",
    "\\text{RI} = \\frac{a + b}{C_2^{n_{samples}}}\n",
    "$$\n",
    "where $C_2^{n_{samples}}$ is the total number of possible pairs in the dataset (without ordering). To adjust for chance, the adjusted Rand index is\n",
    "$$\n",
    "\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-dinner",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:27:41.410384Z",
     "start_time": "2021-07-30T02:27:41.405338Z"
    }
   },
   "source": [
    "References:\n",
    "1. Representative-based clustering jupyter Notebook by CM Alis\n",
    "2. Statquest on k-means https://www.youtube.com/watch?v=4b5d3muPQmA\n",
    "3. https://machinelearningjourney.com/index.php/2020/02/07/k-means-k-medians/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-steal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
